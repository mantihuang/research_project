---
title: "Research Project"
author: "Manting Huang"

execute:
  echo : false
  message: false
  warning: false
  
bibliography: references.bib
---

> **Abstract.**  This research project investigates the effects of unanticipated government spending shocks on output, real exchange rate, and consumption, particularly in the context of the Australian economy. Utilizing quarterly data and the SVAR model, the study captures the dynamic relationships among the endogenous variables. The model estimates impulse response functions (IRFs) and forecast error variance decomposition (FEVDs) for each endogenous variable, allowing for the examination of the dynamic responses of variables and quantification of forecast error variances attributable to the shocks. 
>
> **Keywords.** Government spending shock, exchange rate,impulse responses



# 1. The question objective, and motivation

## Objective
The research project aims to model the effect of government spending shocks on output, real exchange rate and consumption.

## Research question 
How does the unanticipated government spending shock influence output, real exchange rate and consumption in the short run and in the long run?

## Motivation
Most economies have experienced significant shocks due to Covid Pandemic,resulting in a marked increase in government net debt-to-GDP ratios.  For instance,Australian's general government net debt-to-GDP ratio surged from 24.5\% in 2019 to 38.1 in 2021\%  by [@abs2021government], while the United States's ratio rose from 106.1\% to 121.1\% by [@fed2023federal]. 


There has been considerable research on the consequences of government spending shocks on output, consumption, and other crucial macroeconomic factors. Economic theory suggests that positive shocks result in increased output and consumption. However, there has been less emphasis on the economy's external sector, including real exchange rates, imports, and exports. By employing the SVARs model to identify government spending shocks and investigating their connection with these variables, policymakers can enhance the formulation and execution of fiscal and monetary policies in response to unanticipated government spending shocks.

# 2. Data and their properties

## Data 

For this study, quarterly data will be utilized for estimation purposes, as it is more reasonable to assume that government spending can respond within a given period. The primary focus of this research will be on the Australian economy, allowing for an in-depth analysis of its reaction to government spending shocks. It is important to note that the findings may not necessarily be applicable to other countries due to their distinct institutional features and economic attributes. Future research could extend the model to other nations in order to gain a broader understanding of cross-country variations in response to government spending shocks.

Initially, I will load the packages that enable direct data downloads from the Reserve Bank of Australia [@rba2023statistics] and the Federal Reserve Bank of St. Louis [@FRED].

```{r}
library(readabs)
library(readrba)
library(tseries)
library(ggplot2)
library(cowplot)
library(dplyr)
library(zoo)
library(tidyverse)
library(fredr)
library(urca)
library(xts)
library(GIGrvg)
library(mvtnorm)
library(mgcv)
library(ggplot2)
library(gridExtra)
```



## Variables

### Consumption
The dataset contains information on household final consumption expenditure, which is measured in millions of dollars and recorded quarterly, with adjustments for seasonal variations. The plot, however, reveals that the data is non-stationary, with the mean value changing over time. This is a common characteristic of macroeconomic variables. As a result, I also look at the consumption growth data instead, which measures the year-ended household consumption growth and displays a decline in 2020 due to the COVID-19 pandemic. The mean value of consumption growth is 3.41%, indicating that the growth rate fluctuates around this value and is stationary over time.

In the model, the original consumption variable is log-transformed to reduce the scale and conform to the assumption of normality.

```{r}
# Data extraction and cleaning
# Consumption and consumption growhth
consumption.dl   <- read_rba(series_id = "GGDPECCVPSH")
consumption <- to.quarterly(xts(consumption.dl$value, consumption.dl$date), OHLC = FALSE)
consumption_growth <- 100*diff(log(consumption))

# real GDP
realGDP.dl   <- read_rba(series_id = "GGDPCVGDP")
realGDP <- to.quarterly(xts(realGDP.dl$value, consumption.dl$date), OHLC = FALSE)
realGDP_growth <- 100*diff(log(realGDP))

# nominal GDP, seasonally adjusted (in case you decide to use this instead)
nomGDP.dl <- read_abs(series_id = "A2454486X")
nomGDP <- to.quarterly(xts(nomGDP.dl$value, consumption.dl$date), OHLC = FALSE)
nomGDP_growth <- 100*diff(log(nomGDP))



# Trade balance
imports.dl <- read_abs(series_id = "A2454505V")
imports <- to.quarterly(xts(imports.dl$value, consumption.dl$date), OHLC = FALSE)

exports.dl <- read_abs(series_id = "A2454510L") 
exports <- to.quarterly(xts(exports.dl$value, consumption.dl$date), OHLC = FALSE)

# Inflation
CPI.dl <- read_abs(series_id = "A2325846C")
# CPI <- to.quarterly(xts(CPI.dl[45:(nrow(CPI.dl)-1),]$value, consumption.dl$date), OHLC = FALSE)
CPI <- to.quarterly(xts(CPI.dl[45:(nrow(CPI.dl)),]$value, consumption.dl$date), OHLC = FALSE)

CPI_inflation.dl <- read_rba(series_id = "GCPIAGQP")
# CPI_inflation <- to.quarterly(xts(CPI_inflation.dl[149:(nrow(CPI_inflation.dl)-1),]$value, consumption.dl$date), OHLC = FALSE)
CPI_inflation <- to.quarterly(xts(CPI_inflation.dl[149:(nrow(CPI_inflation.dl)),]$value, consumption.dl$date), OHLC = FALSE)

# real exchange rate (TWI)
realTWI.dl   <- read_rba(series_id = "FRERTWI")
# realTWI <- to.quarterly(xts(realTWI.dl[-nrow(realTWI.dl),]$value,consumption.dl[44:nrow(consumption.dl),]$date), OHLC = FALSE) 

realTWI <- to.quarterly(xts(realTWI.dl$value,consumption.dl[44:nrow(consumption.dl),]$date), OHLC = FALSE) 


#realTWI <- to.quarterly(xts(realTWI.dl$value,exports.dl$date), OHLC = FALSE)

# M3
M3.dl   <- read_rba(series_id = "DMAM3N")
M3 <- to.quarterly(xts(M3.dl$value, M3.dl$date), 
                         OHLC = FALSE)
# M3 <- head(M3, -1)

# Government spending, seasonally adjusted
pubcons.dl <- read_abs(series_id = "A2304036K")
pubcons <- to.quarterly(xts(pubcons.dl$value, consumption.dl$date),
                         OHLC = FALSE)

pubinv.dl <- read_abs(series_id = "A2304064V")
pubinv <- to.quarterly(xts(pubinv.dl$value, consumption.dl$date), 
                         OHLC = FALSE)

gov_spend <- pubcons + pubinv
gov_spend_growth <- 100*diff(log(gov_spend))

# matrix of endogenous variables 
# df <-  merge(consumption, realGDP, nomGDP,  realTWI, imports,exports,CPI)
df <-  merge(consumption, realGDP, nomGDP,  realTWI, imports,exports,CPI)
df2 <- merge(df,realTWI, by = "Date")
df$M3 <- as.numeric(M3[-nrow(M3)])
df$gov_spend <- as.numeric(gov_spend)
df <- na.omit(df)
df$Date <- as.vector(index(df))





# transformed in log terms
log_df <- log(df)
log_df$Date <- as.vector(index(log_df))

#growth rates including inflation 
growth_df <- na.omit(merge(consumption_growth, realGDP_growth, nomGDP_growth, CPI_inflation, 
                           gov_spend_growth))
growth_df$Date <- as.vector(index(growth_df))

# set date for 
```



```{r}
# Consumption levels and growth plot
plot1<- ggplot(data = log_df, aes(x = Date, y =consumption)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " Household Consumption (log)", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")
plot2<- ggplot(data = growth_df, aes(x = Date, y =consumption_growth)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " Household Consumption Growth", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")

plot <- plot_grid(plot1, plot2,  ncol = 1, align = "v")
print(plot)

```


### GDP

The real GDP is measured in million dollars, and the plot of this data suggests that it is non-stationary due to its increasing trend. To address this, I transformed the original variable using the lag() function in R, resulting in the GDP percentage change.

When examining the plot of the GDP percentage change, it appears to be more stationary over time. However, the data fluctuates significantly during the COVID period. In a later section, I also run ACF and PACF tests, which suggest that I should keep the original variable.
```{r}
# GDP levels and growth plot

plot2<- ggplot(data = log_df, aes(x = Date, y =realGDP)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " GDP (log)", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")
plot3<- ggplot(data = growth_df, aes(x = Date, y =realGDP_growth)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " GDP percentage change", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")

plot <- plot_grid(plot2, plot3,  ncol = 1, align = "v")
print(plot)

```
### Exchange rate

The real exchange rate data is sourced from the Reserve Bank of Australia (RBA) and is provided on a quarterly basis. RBA uses the Australian dollar trade-weighted index as a measure of the real exchange rate. This index represents the price of the Australian dollar in terms of a group of foreign currencies, based on their share of trade with Australia.
```{r }


# TWI 

plot5<- ggplot(data = df, aes(x = Date, y =realTWI)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " Real exchange rate (TWI)", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")

print(plot5)
```






Additionally, I also investigate the nominal exchange rate variable. I download the AUD/USD exchange rate, as the US dollar is widely accepted as an international currency. (Note: The data obtained using the package may not exactly match the description on the website.) However, since the nominal exchange rate may be influenced by various factors and due to data availability constraints, this research will focus exclusively on the real interest rate (TWI).



### Exports and Imports
As this research aims to identify the government spending shock, exports and imports are also included. The data is season-adjusted. Trade balance is calculated as the difference between exports and imports.


```{r}

plot6<- ggplot(data = log_df, aes(x = Date)) +
  geom_line(aes(y = exports-imports), color = "steelblue", size = 0.8) +
  #geom_line(aes(y = imports), color = "firebrick", size = 0.8) +
  labs(title = " Trade Balance (log)", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")

print(plot6)

```





### government spending data
I will keep the log transformation of the original variable.
```{r}
plot7<- ggplot(data = log_df, aes(x = Date, y =gov_spend)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " Government Spending", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")

plot8<- ggplot(data = growth_df, aes(x = Date, y =gov_spend_growth)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(title = " Government Spending percentage change", x = "Period", y = "Index") +
  theme_bw() +
  theme(plot.title = element_text(size = 16, hjust = 0.5, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 12),
        legend.position = "none")

gov_plot <- plot_grid(plot7, plot8,  ncol = 1, align = "v")
print(gov_plot)

```
    
I also run the ADF test for this variable, and with a p-value of 0.5, we cannot reject the null hypothesis, indicating that the government spending data is expected to be non-stationary. For completeness, I also plot the ACF (autocorrelation function) and PACF (partial autocorrelation function). As shown in the ACF, there is a slow and gradual decay, suggesting that an autoregressive process might be suitable for the data. The PACF results also support using an AR model because the plot has a sharp cut-off.

```{r}
# adf.test(gov_spending_australia$log_gov)
# test1 = acf(gov_spending_australia$log_gov)
# test2 = pacf(gov_spending_australia$log_gov)
```


## preliminary Data Analysis

In this section, I will present a table of the ADF test results for the remaining variables for completeness. As stated in the previous section, I will retain the log transformation of the original variable in my research. Additionally, the PACF and ACF tests suggest using an autoregression model.

### ADF test
 

```{r}
library(tidyverse)
library(urca)
library(xts)

# Convert the 'xts' object to a 'data.frame'
df_data_frame <- as.data.frame(df)[, -which(colnames(df) == "Date")]



# Perform the ADF test on each variable and extract the test statistic and p-value

# this procedure below is nicely coded! But using 1 lag for all the variables is not good. Better too high a lag number than too small. Stationarity of the TWI is most likely only the effect of too few lags used.
adf_summary <- map_df(df_data_frame , function(x) {
  adf_test <- ur.df(x, type = "none", lags = 1)
  adf_test_summary <- summary(adf_test)
  tibble(
    Test_statistic = adf_test_summary@teststat[1, 1],
    Critical_value= adf_test_summary@cval[1, 1]
  )
}, .id = "Variable")

# Perform the test for the first differences to determine the integration order and report in one table please




# Add variable names to the summary table
adf_summary$Variable <- colnames(df_data_frame )

# Display the ADF test summary table
knitr::kable(adf_summary, digits = 3)

```

As anticipated, macroeconomic data is typically non-stationary. The only exception in this case is the real exchange rate, which is stationary.

### PACF and ACF test

```{r}
nrow <- 3
ncol <- 3

# Set up the combined plot grid
par(mfrow = c(3,3))

# Run the ACF and PACF functions for each variable and display the plots
for (i in seq_along(df_data_frame )) {
  variable_name <- colnames(df_data_frame )[i]

  # ACF plot
  acf(df_data_frame [[i]], main = paste("ACF of", variable_name))
}

# Reset the graphical parameters to default
par(mfrow = c(nrow, ncol))

# PACF plot
for (i in seq_along(df_data_frame )) {
  variable_name <- colnames(df_data_frame )[i]

  # PACF plot
  pacf(df_data_frame [[i]], main = paste("PACF of", variable_name))
}



```
# 3. The model 



## Model Specification 

### Structural Form (SF)
The structural model is specified as follows:
$$\begin{align}
B_0y_t &= b_0 + B_1 y_{t-1} + \dots + B_p y_{t-p} + u_t\\
u_{t}| Y_{t-1} &\sim _{iid} ( 0, I_N)
\end{align}$$

### Reduced Form (RF)
Then the empirical model for this study is a structural vector autoregression of the reduced form representation:

$$\begin{align}
y_t &= \mu_0 + A_1 y_{t-1} + \dots + A_p y_{t-p} + \varepsilon_t\\
\text{where }B_0^{-1}u_t &= \varepsilon_t| Y_{t-1} \sim _{iid} ( 0, \Sigma)\\
\Sigma &= B_0^{-1}B_0^{-1'}
\end{align}$$

Where:

$$y_t=\begin{pmatrix}  govspend_t &= \text{government spending}
\\ consump_t  &= \text{consumption}
\\ realGDP_t  &= \text{real GDP}
\\ TWI_t  &= \text{real exchange rate (trade-weighted index)}
\\ CPI_t  &= \text{consumer price index}
\\ export_t  &= \text{exports}
\\ imports_t  &= \text{imports}
\\ M3_t  &= \text{M3 money supply}
\end{pmatrix}$$

$\varepsilon_t$ is a vector of the structural shocks at time t.

Assumptions regarding the model's error terms:
$\varepsilon_t |Y_{t-1}  \sim iid(0_N, Σ)$

## Impusle response function and Forecast error variance decomposition

The research project aims to model the effect of government spending shocks on output, real exchange rate, and consumption. The SVAR model specified above captures the dynamic relationships among the endogenous variables. To answer the research questions, I will use the SVAR model to estimate the impulse response functions (IRFs) and forecast error variance decomposition (FEVDs) for each endogenous variable. With IRFs, I will be able to observe the dynamic response of each variable, while the FEVDs will quantify the portions of the forecast error variance of each variable attributable to the shocks this paper aims to identify.


## Relevant economic context
I would expect the IRF for government spending to initially increase in response to the government spending shock and then decrease to the steady state. Consumption and GDP will also increase, but on a smaller scale compared to government spending. I am uncertain about the real exchange rate. Data shows that the real exchange rate will increase upon the shock and continue to rise after the shock.


# 4. Modelling Framework

## Basic Model
First, I re-write the RF of the model as the following matrix notation:
$$\begin{align*}  Y = XA + E \\ \text{where} E|X \sim MN_{T \times N}(0_{T \times N},\Sigma,I_T)  \end{align*}$$

Let $L(A, \Sigma | Y, X)$ be the likelihood function given by:


$$
L(A, \Sigma | Y, X) \propto \det(\Sigma)^{-\frac{T}{2}} \exp \left\{-\frac{1}{2} \text{tr} \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\}
$$


We can further simplify it as:

$$
L(A, \Sigma | Y, X) \propto \det(\Sigma)^{-\frac{T}{2}} \exp \left\{-\frac{1}{2} \text{tr} \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} \exp \left\{-\frac{1}{2} \text{tr} \left[\Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\}
$$

Let $L(A, \Sigma | Y, X)$ be the likelihood function given by:

$$
L(A, \Sigma | Y, X) \propto \det(\Sigma)^{-\frac{T}{2}} \exp \left\{-\frac{1}{2} \text{tr} \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\}
$$

We can further simplify it as:

$$
L(A, \Sigma | Y, X) \propto \det(\Sigma)^{-\frac{T}{2}} \exp \left\{-\frac{1}{2} \text{tr} \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} \exp \left\{-\frac{1}{2} \text{tr} \left[\Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\}
$$




From Maximum Likelihood Estimation,

$$
\hat{A} = (X'X)^{-1}X'Y
$$

$$
\hat{\Sigma} = \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})
$$


The prior distribution is given by:

$$
p(A, \Sigma) = p(A|\Sigma) \cdot p(\Sigma)
$$

where:

$$
A|\Sigma \sim MN_{K \times N} (\underline{A}, \Sigma , \underline{V})
$$

$$
\Sigma \sim IW_{N}(\underline{S},\underline{\nu})
$$

The parameters are defined as follows:

$$
\underline{A} = \begin{bmatrix} 0_{N \times 1} \\ I_N \\ 0_{N \times (p-1)N} \end{bmatrix}
$$

$$
\text{Var}[vec(A)] = \Sigma \otimes \underline{V}
$$

$$
\underline{V} = \text{diag}([\kappa_2 \quad \kappa_1 (p^{-2} \otimes \imath_N)])
$$

$$
p = [1,2,...,p], \qquad \imath_N = [1,...,1]
$$


The full conditional posterior is given by:

$$
p(A,\Sigma|Y,X) = p(A|Y,X,\Sigma) \cdot p(\Sigma|Y,X)
$$

where:

$$
p(A|Y,X,\Sigma) = MN_{K \times N}(\bar{A}, \Sigma, \bar{V})
$$

$$
p(\Sigma | Y, X) = IW_N(\bar{S},\bar{\nu})
$$

We can derive the full conditional posterior as follows:

$$
P(A,\Sigma|Y,X) \propto L(A,\Sigma|Y,X) \cdot p(A,\Sigma)
$$

$$
\propto L(A,\Sigma|Y,X) \cdot p(A|\Sigma) \cdot p(\Sigma)
$$

$$
\propto \det(\Sigma)^{-\frac{T}{2}} \times \exp \left\{-\frac{1}{2} \text{tr} \left[ \Sigma^{-1}(A-\hat{A})' X'X (A-\hat{A})\right] \right\}
$$

$$
\times \exp\left\{-\frac{1}{2}\text{tr} \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\}
$$

$$
\times \det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}}
$$

$$
\times \exp\left\{-\frac{1}{2}\text{tr} \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
$$

$$
\times \exp \left\{ -\frac{1}{2}\text{tr} \left[ \Sigma^{-1} \underline{S} \right] \right\}
$$

Simplifying further, we have:

$$
p(A,\Sigma|Y,X) \propto \det{(\Sigma)}^{-\frac{T+N+K+\underline{\nu}+1}{2}} \times \exp \left\{-\frac{1}{2}\text{tr} \left[ \Sigma^{-1} \left[(A-\hat{A})^{'}X'X(A-\hat{A})+(A-\underline{A})^{'} \underline{V}^{-1}(A-\underline{A}) + (Y-X\hat{A})^{'}(Y-X\hat{A})+\underline{S}  \right]\right] \right\}
$$

$$
\propto \det{(\Sigma)}^{-\frac{T+N+K+\underline{\nu}+1}{2}} \times \exp\left\{ -\frac{1}{2}\text{tr} \left[ \Sigma^{-1} \left[ (A-\bar{A})^{'} \bar{V}^{-1} (A-\bar{A})+\underline{S} +Y^{'}Y + \underline{A}^{'} \underline{V}^{-1}\underline{A} -\bar{A}^{'} \bar{V}^{-1}\bar{A}\right]\right]\right\}
$$

where:

$$
\bar{V} = (X^{'}X+ \underline{V}^{-1})^{-1}
$$

$$
\bar{A} = \bar{V}(X^{'}Y+\underline{V}^{-1} \underline{A})
$$

$$
\bar{\nu} = T + \underline{\nu}
$$

$$
\bar{S} = \underline{S} + Y^{'}Y +  A^{'}\underline{V}^{-1}\underline{A} - \bar{A}^{'}\bar{V}^{-1}\bar{A}
$$

Based on the derivation, the posterior function is 

```{r, echo=TRUE}
#| code-fold: true
#| code-summary: "Show code"

## Function for drawing posterior distribution , based on lec 12
kappa.1 = 1

Posterior <- function(Y, X, p, N, S,kappa.1) {
  # MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  ############################################################
  kappa.1     = kappa.1
  # kappa.2     = 100
  kappa.2     = 100
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # normal-inverse Wishard posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws 
  ############################################################
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  B.posterior       = array(NA,c(N,N,S))
  L                 = t(chol(V.bar))
  B1.posterior = array(NA,c(N,(1+N*p),S))
  
  for (s in 1:S){
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B.posterior[,,s]= t(cholSigma.s)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    B1.posterior[,,s] =  B.posterior[,,s]%*%t(A.posterior[,,s])
  }
  
  return(list(A.posterior = A.posterior, B.posterior = B.posterior, Sigma.posterior = Sigma.posterior,B1.posterior = B1.posterior))
  
}


```
The sign restriction algorithm is based on @rubio2010structural. For current project, it only restricts the impulse response function at time 0.
```{r ,echo=TRUE}
#| code-fold: true
#| code-summary: "Show code"

SignRestrictions <- function(sign.restrictions,posterior){
  # posterior -   a list - estimation outcome from function Posterior


  A.posterior   = posterior$A.posterior
  B.posterior = posterior$B.posterior
  B1.posterior = posterior$B1.posterior
  Sigma.posterior = posterior$Sigma.posterior

  S = dim(A.posterior)[3]
  N = dim(B.posterior)[1]
  p = (dim(B1.posterior)[2]-1)/N


  R1            = diag(sign.restrictions)
  B0.draws      = array(NA,c(N,N,S))
  B1.draws      = array(NA,c(N,(1+N*p),S))
  
  A.draws       = array(NA,c((1+N*p),N,S))
  B.draws       = array(NA,c(N,N,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0)
for (s in 1:S){

  setTxtProgressBar(pb, s)

  B0.tilde <-B.posterior[,,s]
  B1.tilde <-  B1.posterior[,,s]
  A <- A.posterior[,,s]
  Sigma <- Sigma.posterior[,,s]

  sign.restrictions.do.not.hold = TRUE

  i=1

  while (sign.restrictions.do.not.hold){
    X           = matrix(rnorm(N*N),N,N)
    QR          = qr(X, tol = 1e-10)
    Q           = qr.Q(QR,complete=TRUE)
    R           = qr.R(QR,complete=TRUE)
    Q           = t(Q %*% diag(sign(diag(R))))
    B0          = Q%*%B0.tilde
    B0.inv      = solve(B0)
    # check       = prod(R1 %*% B0.inv %*% diag(N)[,1] > 0) # Check reponse at time 0
    check       = prod(R1 %*% B0.inv %*% diag(N)[,1] >= 0)


    if (check==1){sign.restrictions.do.not.hold=FALSE}
    i=i+1
  }
  B1            = Q%*%B1.tilde
  A.draws[,,s]  = t(solve(B0)%*%B1)
  B.draws[,,s] =  solve(B0)
  B0.draws[,,s] = B0
  B1.draws[,,s] = B1

}

  return (list(B0.draws = B0.draws,
               B1.draws = B1.draws,
               A.draws  = A.draws, # For empirical part
               B.draws  = B.draws, # For empirical part
               i        = i))
}

```



## Hyperparamter extention
Now assume that 
$$A|\Sigma \sim MN_{K \times N} (\underline{A}, \Sigma , \underline{\kappa}_A \underline{V}) $$
where : 

$$\kappa_A \sim G\left(\underline{s}_A,\frac{1}{2}\right)$$.

Then the posterior of $\kappa_A$ is calculated as follows:


$$
P(\kappa_A |Y,X,A) = L(Y|X,A,\Sigma) \times P(\kappa_A) \times P(A|\Sigma,\kappa_A) \times P(\Sigma)  
$$

$$
\propto p(\kappa_A) \times P(A|\Sigma,\kappa_A) 
$$

$$
= \kappa_A^{\frac{1}{2}-1} \exp\left\{ -\frac{\kappa_A}{\underline{s}_A}\right\} \times \det(\kappa_A \underline{V})^{-\frac{N}{2}} \times \exp \left\{-\frac{1}{2} \text{tr}[\Sigma ^{-1} (A - \underline{A})' (\kappa_A \underline{V})^{-1}] (A - \underline{A}) \right\} 
$$

$$
= \kappa_A ^{\frac{1}{2} - \frac{KN}{2} - 1} \times \exp \left\{-\frac{1}{2} \left[\frac{1}{\kappa_A}\text{tr} \Sigma^{-1}(A - \underline{A})'(\underline{V}^{-1})(A - \underline{A})\right] + \frac{2}{s_A}\kappa_A \right\}
$$



which it is the kernel of Generalized Inverse Gaussian (GIG) distribution with 

$$\lambda = \frac{1}{2} - \frac{KN}{2} $$ 
$$\chi = tr (\Sigma^{-1}(A - \underline{A})'(\underline{V}^{-1})(A - \underline{A})) $$

$$\psi = \frac{2}{s_A}$$


Gibbs Sampler : 

Initialize $\kappa_A$ at $\kappa_A^{(0)}$: 

At each iteration s:

1. Draw $(A,\Sigma)^{(s)} \sim p(A,\Sigma|Y,X, \kappa_A^{s-1}$
2. Draw $\kappa_A^{s} \sim p(\kappa_A |Y,X,A,\Sigma)$

Repeat step 1 and 2 for S1+S2 times and then discard the first S1. 

```{r,echo=TRUE}
#| code-fold: true
#| code-summary: "Show code"

kappa.1 = 1
Posterior_kappaA <- function(Y, X, p, N, S1,S2, kappa.1) {
  # MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

  
  kappaA           <- c()
  
  # prior distribution
  ############################################################
  # Initialize kappa_A
  kappaA[1] = 1
  kappas = 0.1
  kappani = 1/2
  
  kappa.1     = kappa.1
  # kappa.2     = 100
  kappa.2     = 100
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # normal-inverse Wishard posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = kappaA*(V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior))
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws 
  ############################################################
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  B.posterior       = array(NA,c(N,N,S))
  L                 = t(chol(V.bar))
  B1.posterior = array(NA,c(N,(1+N*p),S))
  
  for (s in 1:(S1+S2)){
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B.posterior[,,s]= t(cholSigma.s)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    B1.posterior[,,s] =  B.posterior[,,s]%*%t(A.posterior[,,s])
    # draw kappa_A
    lambda = 1/2 - p*N/2
    chi = sum(diag(solve(Sigma.posterior[,,s]) %*% t(A.posterior[,,s] - A.prior) %*% solve(V.prior) %*% (A.posterior[,,s] - A.prior)))
    psi = 2/kappas
    kappaA[s] = GIGrvg::rgig(n=1,lambda,chi,psi)
    
  }
  
  return(list(A.posterior = A.posterior[,,(S1+1):(S1+S2)], 
              B.posterior = B.posterior[,,(S1+1):(S1+S2)], 
              Sigma.posterior = Sigma.posterior[,,(S1+1):(S1+S2)],
              B1.posterior = B1.posterior[,,(S1+1):(S1+S2)],
              kappaA = kappaA[(S1+1):(S1+S2)]))
  
}


```

## Narrative Sign Restriction extention
The extended model adopts the narrative sign restriction in (@antolin-diaz2018narrative). In this extension, the focus is on imposing restrictions on the signs of the Structural Shocks. The concept revolves around constraining the sign of specific structural shocks during a particular time period, guided by the narrative information.

The posterior distribution is the same as the basic model. The difference is on the sign restriction. Based on the original sign restriction, the structural shocks for each period can be calculated by:

$$\begin{align*} u_t = B_0 y_t - B_1 y_{t-1} - .. - b_0 \end{align*}$$


Then we could put the restrictions on the sign of $u_t$.


```{r, echo=TRUE}
#| code-fold: true
#| code-summary: "Show code"

NarrativeSignRestrictions <- function(sign.restrictions,posterior,narrative.restrictions,Y,X){
  # posterior -   a list - estimation outcome from function Posterior
  
  
  A.posterior   = posterior$A.posterior
  B.posterior = posterior$B.posterior
  B1.posterior = posterior$B1.posterior
  Sigma.posterior = posterior$Sigma.posterior
  YY = Y
  XX = X
  
  S = dim(A.posterior)[3]
  N = dim(B.posterior)[1]
  p = (dim(B1.posterior)[2]-1)/N
  
  
  R1            = diag(sign.restrictions)
  B0.draws      = array(NA,c(N,N,S))
  B1.draws      = array(NA,c(N,(1+N*p),S))
  A.draws       = array(NA,c((1+N*p),N,S))
  B.draws       = array(NA,c(N,N,S))

  pb = txtProgressBar(min = 0, max = S, initial = 0)
  for (s in 1:S){
    
    setTxtProgressBar(pb, s)
    
    B0.tilde <-B.posterior[,,s]
    B1.tilde <-  B1.posterior[,,s]
    A <- A.posterior[,,s]
    Sigma <- Sigma.posterior[,,s]
    
    sign.restrictions.do.not.hold = TRUE
    narrative.restriction.do.not.hold = TRUE
    i=1
    
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N*N),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde 
      B0.inv      = solve(B0) 
      B1            = Q%*%B1.tilde
      u = YY %*% t(B0) - XX %*% t(B1)
      u= u[,1]
      check       = prod(R1 %*% B0.inv %*% diag(N)[,1] >= 0  ) # Check reponse at time 0
      check_narrative = prod(sign(u[narrative.restrictions]) > 0)
      
      if (check == 1 && check_narrative == 1) {
        sign.restrictions.do.not.hold = FALSE
      }
      i = i + 1

      B0.draws[, , s] = B0
      B1.draws[, , s] = B1
      A.draws[,,s]  = t(solve(B0)%*%B1)
      B.draws[,,s] =  solve(B0)
    }
    
    close(pb)
    
  }
  
  return (list(B0.draws = B0.draws,
               B1.draws = B1.draws,               
               A.draws  = A.draws,
               B.draws  = B.draws,
               i        = i))
}

```
## Stochastic Volatility 

This section takes heteroskedasticity into consideration, specifically Stochastic Volatility conditional heteroskedasticity. Current methods adopts @omori2007stochastic, which proposes to proximate the log chi-squared distribution with
one degree of freedom by a mixture of ten normal distributions.
$$
Y = XA + E
$$
$$
E|X \sim N_T(0_T,diag(\sigma^2))
$$

$$
\sigma^2 = (exp\{h_1\}, ... , exp\{h_t\})
$$
$h_t$ - follows a Stochastic Volatility process


The full conditional posterior distribution for $\alpha,\Sigma$ is given by
$$
P(A,\Sigma|Y,X,h) = MN (\bar{\alpha},\bar{V},\bar{S},\bar{\nu})
$$

where
$$
\bar{V}=(X'diag(h)^{-1}X+\underline{V}^{-1})^{-1}
$$

$$
\bar{\alpha}=\bar{V}(X'diag(h)^{-1}Y+\underline{V}^{-1}\underline{\alpha})
$$

$$
\bar{S}=\underline{V}+Y'diag(h)^{-1}Y+\underline{\alpha'}\underline{V}^{-1}\underline{\alpha}-\bar{\alpha'}\bar{V}^{-1}\bar{\alpha}
$$

$$
\bar{\nu}= T + \underline{\nu}
$$

Gibbs Sampler :
At each iteration s,

Step 1. Sample $h^{S}$ from the distribution $P(h|Y,X,A, \Sigma)$
Step 2. Sample $\alpha^{S},\Sigma^{s}$ from the $P(A,\Sigma|Y,X,h) = MN (\bar{\alpha},\bar{V},\bar{S},\bar{\nu})$


```{r,echo=TRUE}
#| code-fold: true
#| code-summary: "Show code"
SVcommon.Gibbs.iteration = function(S,Y,X, priors){
  
  # priors is a list containing:
  #   h0.v - a positive scalar
  #   h0.m - a scalar
  #   sigmav.s - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH - a TxT matrix
  T             = dim(Y)[1]
  N             = dim(Y)[2]
  K             = dim(X)[2] 
  
aux     = list(
  Y           = Y,
  X           = X,
  H           = matrix(1,T,1),
    h0          = 0,
  sigma.v2      = 1,
  s           = matrix(1,T,1),
  A          = matrix(0, K, N),
  Sigma      = matrix(0, K, N),
  sigma2    = matrix(1,T,1)
)
  
  
  posteriors      = list(
    H           = matrix(NA,T,S),
    sigma2      = matrix(NA,T,S),
    s           = matrix(NA,T,S),
    h0          = rep(NA,S),
    sigma.v2    = rep(NA,S),
    A           = array(NA, c(K,N,S)),
    Sigma       = array(NA, c(N,N,S))
  )
  

  


  
    # Sampling A and Sigma
  kappa.1     = 1
  kappa.2     = 100
  kappa.3     = 1
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

  # Prior distribution specification - Minnesota prior 

  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N + 1), ] = diag(N)

  S.prior     = diag(diag(Sigma.hat))
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  
  nu.prior    = 8
  

    
  #####
  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
  
for (s in 1:S){
  # normal-inverse Wishard posterior parameters
  
  V.bar.inv   = t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(aux$X)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(aux$Y) + nu.prior
  S.bar       = S.prior + t(aux$Y)%*%diag(1/as.vector(aux$sigma2))%*%aux$Y +   t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws 
  ############################################################

  Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)
  aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))
  A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))
  L                      = t(chol(V.bar))
  aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])

  

  # Lambda        = solve(chol(S.prior))
  Lambda        = solve(chol(aux$Sigma[,,1]))
  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
  Y.tilde       = as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
  
  # sampling initial condition
  ############################################################
  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))
  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))
  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0        = h0.draw
  
  # sampling sigma.v2
  ############################################################
  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2  = sigma.v2.draw
  
  # sampling auxiliary states
  ############################################################
  Pr.tmp        = simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         = t(apply(Pr, 1, cumsum))
  r             = matrix(rep(runif(T), 10), ncol = 10)
  ss            = apply(s.cum < r, 1, sum) + 1
  aux$s         = as.matrix(ss)
  
  
  # sampling log-volatilities using functions for tridiagonal precision matrix
  ############################################################
  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH
  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
  lead.diag     = diag(D.inv)
  sub.diag      = mgcv::sdiag(D.inv, -1)
  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           = diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             = as.matrix(rnorm(T))
  a             = forwardsolve(D.L, b)
  draw          = backsolve(t(D.L), a + x)
  aux$H         = as.matrix(draw)
  aux$sigma2    = as.matrix(exp(draw))
  


  



      posteriors$H[,s]             = aux$H
      posteriors$sigma2[,s]        = aux$sigma2
      posteriors$s[,s]             = aux$s
      posteriors$h0[s]             = aux$h0
      posteriors$sigma.v2[s]       = aux$sigma.v2
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
  }

  return(posteriors)
}

```





## Simulations 
This section aims to evaluate the code by generating artificial data consisting of 1000 observations simulated from a bivariate Gaussian random walk process. The covariance matrix of the process is set to be the 2x2 identity matrix. 

```{r, echo=FALSE}
### Proof that your model can replicate the true parameters of the data-generating process.


# Set parameters
n_obs = 1000
p <- 1
N <- 2
S <- 50000

# 1. Generate artificial data
set.seed(2023) # for reproducibility
cov_matrix <- diag(N)
mean_vector <- rep(0, N)

# Simulate bi-variate Gaussian random walk
RW1 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)
RW2 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)
y = cbind(RW1,RW2) 

y <- ts(y)

# 2. Estimate the model
# Create Y and X
Y <- y[(p + 1):n_obs, ]
X <- matrix(1, nrow(Y), 1)

for (i in 1:p) {
  X <- cbind(X, y[(p + 1 - i):(n_obs - i), ])
}
plot(y)


```
### Basic Model
I first test my sign Restriction function with the simulated data. The sign restriction here is both positive. 

| Restriction |  + | + |

The A and $\Sigma$ is presented below, suggesting the basic model is working.

The first row of A is close to zero, implying a zero constant and the matrix is close to identity matrix.


```{r}
posterior <- Posterior(Y, X, p, N, S,kappa.1)



################### Apply Sign Restriction ########################

sign.restrictions = c(1,1)
Test.Restriction = SignRestrictions(sign.restrictions,posterior)

B0.draws = Test.Restriction$B0.draws
B1.draws = Test.Restriction$B1.draws


###############################################
A.check <- array(NA,c(N+1,N,S))
S.check <- array(NA,c(N,N,S))

for (s in 1:S){
  # convert Bo into Sigma 
  S.check[,,s] <- B0.draws[,,s] %*% t(B0.draws[,,s])
  A.check[,,s] <- t(B1.draws[,,s]) %*% B0.draws[,,s]
}
Acheck=round(apply(A.check,1:2,mean),4)
Scheck =round(apply(S.check,1:2,mean),4)

```

```{r, echo=TRUE}
Acheck
Scheck
```
### Hyperparameter extention

The distribution of $\kappa_A$ is shown as the following: 
```{r}
S1=5000
S2 = 45000
posterior_k <- Posterior_kappaA(Y, X, p, N, S1,S2 ,kappa.1)


Kappa_A = posterior_k$kappaA
 hist(Kappa_A,
     main=expression(paste("Distribution of ",kappa_A, " draws")),
     freq=FALSE,
     xlab = expression(kappa),
     breaks = 1000,
     xlim = c(0,0.01),
     col = "turquoise4",
     lty = "blank")

```
### Stochastiv Volatility


```{r}

################### Apply Sign Restriction ########################

sign.restrictions = c(1,1)
Test.Restriction = SignRestrictions(sign.restrictions,posterior_k)

B0kappa.draws = Test.Restriction$B0.draws
B1kappa.draws = Test.Restriction$B1.draws


###############################################
Akappa.check <- array(NA,c(N+1,N,S2))
Skappa.check <- array(NA,c(N,N,S2))

for (s in 1:S2){
  # convert Bo into Sigma 
  Skappa.check[,,s] <- B0kappa.draws[,,s] %*% t(B0kappa.draws[,,s])
  Akappa.check[,,s] <- t(B1kappa.draws[,,s]) %*% B0kappa.draws[,,s]
}
Akappacheck=round(apply(A.check,1:2,mean),4)
Skappacheck =round(apply(S.check,1:2,mean),4)

```
```{r,echo=TRUE}

Akappacheck
Skappacheck
```
As can be seen from the results above, the hyperparameter extension also works.

### Narrative Sign Restriction
For this section, I assume the the sign of the structural shocks for period period 65 to period 67 is negative, which is also consistent with the original sign restriction. The parameter values for A and $\sigma$ suggests the function is working well.



```{r}
######## Narrative sign 
narrative.restrictions = c(65:67)

Narrative.Restriction = NarrativeSignRestrictions(sign.restrictions,posterior,narrative.restrictions,Y,X)

B01.draws = Narrative.Restriction$B0.draws
B11.draws = Narrative.Restriction$B1.draws


###############################################
AN.check <- array(NA,c(N+1,N,S))
SN.check <- array(NA,c(N,N,S))



for (s in 1:S){
  # convert Bo into Sigma 
  SN.check[,,s] <- B01.draws[,,s] %*% t(B01.draws[,,s])
  AN.check[,,s] <- t(B11.draws[,,s]) %*% B01.draws[,,s]
}

AN = round(apply(A.check,1:2,mean),4)
SN = round(apply(S.check,1:2,mean),4)

```
```{r,echo=TRUE}
AN
SN
```


# 5. Empirical Estimation

Sign Restrictions are taken as the following:

1. Government spending: A positive government spending shock increases government spending (govspend).

2. Consumption:  Generally, a positive government spending shock could potentially increase consumption. This is due to the multiplier effect. As the government spends more, this increases income for businesses and households, which can then lead to higher consumption. 

3. Real GDP: A positive government spending shock is typically expected to increase real GDP, at least in the short term. Government spending is a component of GDP (which is the sum of consumption, investment, government spending, and net exports). Therefore, if government spending increases, all else being equal, real GDP should also increase.

4. Real exchange rate: The effect is unclear and thus I put 0.

5. Consumer price index: When there is a positive government spending shock, standard New Keynesian model predicts it is inflationary. Although @jorgensen2022inflation shows that in response to government spending shock, inflation could be flat or negative if the economy is hitting the zero lower bound, which goes beyond the scope of this research, current research does not take this into consideration. 

6. M3 money supply: Since the government financed its spending through borrowing, this could increase the money supply.

7. Imports and Exports: Evidence about the IRF for imports and exports is limited thus I place no restrictions (@blagrave2018crossborder) .

|                     |  Sign   |
|---------------------|---------|
| Government Spending | 1       |
| Consumption         | 1       |
| Real GDP            | 1       |
| Real Exchange Rate  | 0       |
| CPI                 | 1       |
| M3                  | 1       |
| Imports             | 0       |
| Exports             | 0       |


## Baseline Model


```{r, echo=FALSE}
library(HDInterval)
set.seed(123456)
########IRF
p = 5 # time lagt0
N = 8
S = 1000

# Create Y and X

y = ts(df[, !names(df) %in% c("Date","nomGDP")])
y = log(y)

Y = y[(p+1):211,]

X       = matrix(1,nrow(Y),1)

for (i in 1:p){
  X     = cbind(X,y[(p+1):211-i,])
}

results = Posterior(Y,X,p,N,S,1)


Sign_restritions = SignRestrictions(sign.restrictions = c(1,1,0,0,0,1,1,1), results)


##################### Results from the Sign_restriction ###################


A.posterior = Sign_restritions$A.draws
B.posterior = Sign_restritions$B.draws
```

The mean and standard deviations for matrix A are reported below:
```{r}


# Calculate the means
A4check_mean <- head(round(apply(Sign_restritions$A.draws, 1:2, mean),4))
# Calculate the standard deviations
A4check_sd <- head(round(apply(Sign_restritions$A.draws, 1:2, sd),4))
```

```{r,echo=TRUE}
A4check_mean 
A4check_sd
```
The mean and standard deviations for matrix B are reported below:
```{r}
# Calculate the means
B4check_mean <- head(round(apply(Sign_restritions$B.draws, 1:2, mean),4))
B4check_sd   <- head(round(apply(Sign_restritions$B.draws, 1:2, sd),4))
# Calculate the standard deviations
```
```{r,echo=TRUE}
B4check_mean 
B4check_sd
```
The third element of Y is realTWI and the eighth element of Y is government spending.
```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(Sign_restritions$A.draws[3,,][8,], xlab = "Simulation times S", ylab = expression(A[38]), col = "#CC66CC")    
hist(Sign_restritions$A.draws[3,,][8,], xlab = expression(A[38]), col = "#CC66CC", main = '')
plot.ts(Sign_restritions$B.draws[3,,][8,], xlab = "Simulation times S", ylab = expression(B[38]), col = "lightblue")    
hist(Sign_restritions$B.draws[3,,][8,], xlab = expression(A[38]), col = "lightblue", main = '')

```

```{r}

########https://www.sciencedirect.com/science/article/pii/S0014292121002634#####


h = 20



IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

for (s in 1:S){
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior


# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1.RData")
```

```{r, echo=FALSE}
# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"
purple = "#b02442"
mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)

# Plots of responses to domestic monetary policy shock
############################################################
load("irf-fevd-k1.RData")
IRFs.k1           = apply(IRF.posterior[,1,,],1:2,mean) 

######1st shock is government  spending shock

IRFs.inf.k1       = apply(IRF.inf.posterior[,1,],1,mean)
rownames(IRFs.k1) = colnames(Y)

IRFs.k1.hdi    = apply(IRF.posterior[,1,,],1:2,hdi, credMass=0.68)
hh          = 1:h+1


par(mfrow=c(3,3), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
  # par(mfrow=c(4,2), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  # if (n==N-1 | n==N){
    # axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  # } else {
    # axis(1,c(1,2,5,9),c("","","",""))
  # }
  axis(1, at = c(12), labels = c("2.5year"))
  
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}

```
The impulse response function (IRF) is depicted over a time span of 5 years with the posterior mean and 68% highest density interval. As anticipated, the impulse response aligns with the specified sign restriction, exhibiting the same sign as intended. Specifically, in the case of the real exchange rate, it initially depreciates and subsequently appreciates. The real exchange rate fully recovers to its pre-shock level within a period of 2.5 years.


In response to a positive government spending shock, both imports and exports initially decrease. However, they gradually recover to their pre-shock levels within half a year. After the recovery period, both imports and exports continue to increase beyond their pre-shock levels. This indicates a positive long-term effect of the government spending shock on both imports and exports, leading to sustained growth in international trade.


Following the shock, both consumption and real GDP display persistence, meaning they continue to exhibit sustained levels of activity even after the initial shock. 

In contrast, the Consumer Price Index (CPI), M3 (a measure of the money supply), and government spending experience an initial increase in response to the shock. However, these variables subsequently begin to decrease after the initial increase. This suggests that the initial surge in CPI, M3, and government spending eventually tapers off, resulting in a downward trend.



```{r}
# Plots of FEVD of Australian rgdp and p

############################################################

hh <- 1:(h + 1)
fevd.au.rgdp <- apply(FEVD.posterior[2,,,], 1:2, mean)
fevd.au.rgdp <- rbind(rep(0, h + 1), apply(fevd.au.rgdp, 2, cumsum))

fevd.realTWI <- apply(FEVD.posterior[3,,,], 1:2, mean)
fevd.realTWI <- rbind(rep(0, h + 1), apply(fevd.realTWI, 2, cumsum))

fevd.exports <- apply(FEVD.posterior[5,,,], 1:2, mean)
fevd.exports <- rbind(rep(0, h + 1), apply(fevd.realTWI, 2, cumsum))

colors <- c("deepskyblue1","deepskyblue2","deepskyblue","deepskyblue3","deepskyblue4","dodgerblue",
            "maroon1","maroon","maroon2","magenta","maroon3","maroon4")



# Set up the layout for two plots side by side
 par(mfrow = c(1, 2), mar = rep(4, 4), cex.axis = 1, cex.lab = 0.8)

# Plotting fevd-au-gdp-k1

plot(hh, fevd.au.rgdp[1,], type = "n", ylim = c(0, 100), axes = FALSE, xlab = "", ylab = "")
# Define labels vector for x-axis (axis 1)
x_axis_labels <- rep("", length(hh))  # Start with a vector of empty strings of the same length as 'hh'
x_axis_labels[c(2, 5, 9)] <- c("1 quarter", "1 year", "2 years")  # Place your labels at desired positions

# Replace the labels in your axis() calls with the revised labels vector
axis(1, hh, x_axis_labels)

# axis(1, hh, c("", "1 quarter", "", "", "1 year", "", "", "", "2 years"))
axis(2, c(0, 50, 100), c("", "FEVD[au.rgdp]", ""))
for (n in 1:N) {
  polygon(c(hh, (h + 1):1), c(fevd.au.rgdp[n, hh], fevd.au.rgdp[n + 1, (h + 1):1]), col = colors[n], border = colors[n])
}
axis(4, (0.5 * (fevd.au.rgdp[1:n, 9] + fevd.au.rgdp[2:n+1, 9])))

# Plotting fevd-au-p-k1
plot(hh, fevd.realTWI[1,], type = "n", ylim = c(0, 100), axes = FALSE, xlab = "", ylab = "")
axis(1, hh, x_axis_labels)
axis(2, c(0, 50, 100), c("", "FEVD[realTWI]", ""))
for (n in 1:N) {
  polygon(c(hh, (h + 1):1), c(fevd.realTWI[n, hh], fevd.realTWI[n + 1, (h + 1):1]), col = colors[n], border = colors[n])
}
axis(4, (0.5 * (fevd.realTWI[1:n, 9] + fevd.realTWI[2:n+1, 9])))
```
The current analysis focuses on identifying one specific shock; however, it is important to consider that other potential shocks could also contribute to explaining the observed variance. Further analysis using FEVD (Forecast Error Variance Decomposition) can shed light on the impact of these alternative shocks. As can be seen from the plot, each shock contributes almost equally to the overall variation.

## Hyperparamter extention 
```{r}

Posterior_kappa = Posterior_kappaA(Y, X, p, N, S1=200 ,S2=800 ,kappa.1)

 
Sign_restritions_kappa = SignRestrictions(sign.restrictions = c(1,1,0,0,0,1,1,1), Posterior_kappa)


```
The hyperparameter analysis is utilized in this section, and it is evident from the IRFs and FEVD plot provided below that the outcomes closely resemble those of the baseline model.
```{r}
A.posterior = Sign_restritions_kappa$A.draws
B.posterior = Sign_restritions_kappa$B.draws
```



The mean and standard deviations for matrix A are reported below:
```{r}
# Calculate the means
Akappacheck_mean <- head(round(apply(A.posterior, 1:2, mean),4))

# Calculate the standard deviations
Akappacheck_sd <- head(round(apply(A.posterior, 1:2, sd),4))
```
```{r,echo=TRUE}
Akappacheck_mean 
Akappacheck_sd

```
The mean and standard deviations for matrix B are reported below:
```{r}
# Calculate the means
Bkcheck_mean <- head(round(apply(B.posterior, 1:2, mean),4))
Bkcheck_sd   <- head(round(apply(B.posterior, 1:2, sd),4))
# Calculate the standard deviations
```
```{r,echo=TRUE}
Bkcheck_mean 
Bkcheck_sd
```
```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior[3,,][8,], xlab = "Simulation times S", ylab = expression(A[38]), col = "#CC66CC")    
hist(A.posterior[3,,][8,], xlab = expression(A[38]), col = "#CC66CC", main = '')
plot.ts(B.posterior[3,,][8,], xlab = "Simulation times S", ylab = expression(B[38]), col = "lightblue")    
hist(B.posterior[3,,][8,], xlab = expression(A[38]), col = "lightblue", main = '')

```
```{r}
########https://www.sciencedirect.com/science/article/pii/S0014292121002634#####

S = 800
h = 20



IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

for (s in 1:S){
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior


# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1_narrative.RData")

load("irf-fevd-k1_narrative.RData")
IRFs.k1           = apply(IRF.posterior[,1,,],1:2,mean) 

######1st shock is government  spending shock

IRFs.inf.k1       = apply(IRF.inf.posterior[,1,],1,mean)
rownames(IRFs.k1) = colnames(Y)

IRFs.k1.hdi    = apply(IRF.posterior[,1,,],1:2,hdi, credMass=0.68)
hh          = 1:h+1


par(mfrow=c(3,3), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
  # par(mfrow=c(4,2), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  # if (n==N-1 | n==N){
  #   axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  # } else {
  #   axis(1,c(1,2,5,9),c("","","",""))
  # }
    axis(1, at = c(12), labels = c("2.5year"))
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
```

```{r}
# Plots of FEVD of Australian rgdp and p

############################################################

hh <- 1:(h + 1)
fevd.au.rgdp <- apply(FEVD.posterior[2,,,], 1:2, mean)
fevd.au.rgdp <- rbind(rep(0, h + 1), apply(fevd.au.rgdp, 2, cumsum))

fevd.realTWI <- apply(FEVD.posterior[3,,,], 1:2, mean)
fevd.realTWI <- rbind(rep(0, h + 1), apply(fevd.realTWI, 2, cumsum))

fevd.exports <- apply(FEVD.posterior[5,,,], 1:2, mean)
fevd.exports <- rbind(rep(0, h + 1), apply(fevd.realTWI, 2, cumsum))
# 
# colors <- c("deepskyblue1","deepskyblue2","deepskyblue","deepskyblue3","deepskyblue4","dodgerblue",
#             "maroon1","maroon","maroon2","magenta","maroon3","maroon4")



# Set up the layout for two plots side by side
 par(mfrow = c(1, 2), mar = rep(4, 4), cex.axis = 1, cex.lab = 0.8)

# Plotting fevd-au-gdp-k1

plot(hh, fevd.au.rgdp[1,], type = "n", ylim = c(0, 100), axes = FALSE, xlab = "", ylab = "")
# Define labels vector for x-axis (axis 1)
x_axis_labels <- rep("", length(hh))  # Start with a vector of empty strings of the same length as 'hh'
x_axis_labels[c(2, 5, 9)] <- c("1 quarter", "1 year", "2 years")  # Place your labels at desired positions

# Replace the labels in your axis() calls with the revised labels vector
axis(1, hh, x_axis_labels)

# axis(1, hh, c("", "1 quarter", "", "", "1 year", "", "", "", "2 years"))
axis(2, c(0, 50, 100), c("", "FEVD[au.rgdp]", ""))
for (n in 1:N) {
  polygon(c(hh, (h + 1):1), c(fevd.au.rgdp[n, hh], fevd.au.rgdp[n + 1, (h + 1):1]), col = colors[n], border = colors[n])
}
# axis(4, (0.5 * (fevd.au.rgdp[1:n, 9] + fevd.au.rgdp[2:n+1, 9]))[c(1, 3, 10)], c("Government spending shock", "", "au.mps"))
axis(4, (0.5 * (fevd.au.rgdp[1:n, 9] + fevd.au.rgdp[2:n+1, 9])))
# Plotting fevd-au-p-k1
plot(hh, fevd.realTWI[1,], type = "n", ylim = c(0, 100), axes = FALSE, xlab = "", ylab = "")
axis(1, hh, x_axis_labels)
axis(2, c(0, 50, 100), c("", "FEVD[realTWI]", ""))
for (n in 1:N) {
  polygon(c(hh, (h + 1):1), c(fevd.realTWI[n, hh], fevd.realTWI[n + 1, (h + 1):1]), col = colors[n], border = colors[n])
}
# axis(4, (0.5 * (fevd.realTWI[1:n, 9] + fevd.realTWI[2:n+1, 9]))[c(3, 10)],)
axis(4, (0.5 * (fevd.realTWI[1:n, 9] + fevd.realTWI[2:n+1, 9])))
```


## Narrative information


According to the perspective provided by @laurie2008perspective, there are three distinct time periods that are widely recognized for government spending expansion before year 2008. These periods are as follows:

1. Whitlam Government, 1974-75: The analysis highlights the increased government spending during the Whitlam Government in 1974-75 as one of the significant periods of expansion.

$\implies$
Narrative Sign Restriction 1 : The government spending shock must take positive values in 1974.
 
2. Recessions in 1982-83: The paper identifies the recessions in 1982-83 as another period characterized by increased government spending.

$\implies$
Narrative Sign Restriction 2 : The government spending shock must take positive values in 1982.

3. Recessions in 1990-91: The perspective also emphasizes the increased government spending that occurred following the recessions in 1990-91.

$\implies$
Narrative Sign Restriction 3 : The government spending shock must take positive values in 1990.

Following the year 2008, there is no consensus regarding the direction of the government spending shock. Nonetheless, in response to the Global Financial Crisis, the Australian government implemented a stimulus package.

1. The Australian government made an announcement in October 2008 regarding a fiscal stimulus package called the Economic Security Strategy, which had a value of AUD 10.4 billion.


2. In February 2009, as the global economy continued to decline, the government introduced a second stimulus package known as the Nation Building and Jobs Plan, amounting to AUD 42 billion.

$\implies$
Narrative Sign Restriction 4: The government spending shock should have positive values in the first quarter of 2009.

Moreover, in response to Covid-19 pandemic, following three programs also had been implemented.

1. JobKeeper Payment: This wage subsidy program was announced on March 30, 2020, and initially intended to run for six months. It was later extended twice and eventually ended on March 28, 2021.
2. Cash Flow Boost: This measure was announced on March 22, 2020, as part of the government's second Economic Response Package. Eligible businesses received payments from late March 2020, and the program ran until June 2020.
3. Coronavirus Supplement: This supplement was announced on March 22, 2020. It began on April 27, 2020, and originally planned to run until September 24, 2020, but was extended twice with a reduced amount. The supplement finally ended on March 31, 2021.

$\implies$
Narrative Sign Restriction 5: The government spending shock should have positive values in the first quarter of 2021.

```{r}

results = Posterior(Y,X,p,N,S,1)
Narrative_Restriction = NarrativeSignRestrictions(sign.restrictions = c(1,1,0,0,0,1,1,1), results,c(16,50,80,156,204),Y,X)
S = 800


##############################################
A.posterior = Narrative_Restriction$A.draws
B.posterior = Narrative_Restriction$B.draws

```
The mean and standard deviations for matrix A are reported below:
```{r}
# Calculate the means
Ancheck_mean <- head(round(apply(A.posterior, 1:2, mean),4))

# Calculate the standard deviations
Ancheck_sd <- head(round(apply(A.posterior, 1:2, sd),4))
```

```{r,echo=TRUE}
Ancheck_mean 
Ancheck_sd

```
The mean and standard deviations for matrix B are reported below:
```{r}
# Calculate the means
Bncheck_mean <- head(round(apply(B.posterior, 1:2, mean),4))
Bncheck_sd   <- head(round(apply(B.posterior, 1:2, sd),4))
# Calculate the standard deviations
Bncheck_mean 
Bncheck_sd
```
```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior[3,,][8,], xlab = "Simulation times S", ylab = expression(A[38]), col = "#CC66CC")    
hist(A.posterior[3,,][8,], xlab = expression(A[38]), col = "#CC66CC", main = '')
plot.ts(B.posterior[3,,][8,], xlab = "Simulation times S", ylab = expression(B[38]), col = "lightblue")    
hist(B.posterior[3,,][8,], xlab = expression(A[38]), col = "lightblue", main = '')

```

```{r}
########https://www.sciencedirect.com/science/article/pii/S0014292121002634#####


h = 20



IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

for (s in 1:S){
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior


# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1_narrative.RData")

load("irf-fevd-k1_narrative.RData")
IRFs.k1           = apply(IRF.posterior[,1,,],1:2,mean) 

######1st shock is government  spending shock

IRFs.inf.k1       = apply(IRF.inf.posterior[,1,],1,mean)
rownames(IRFs.k1) = colnames(Y)

IRFs.k1.hdi    = apply(IRF.posterior[,1,,],1:2,hdi, credMass=0.68)
hh          = 1:h+1


par(mfrow=c(3,3), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
  # par(mfrow=c(4,2), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  # if (n==N-1 | n==N){
  #   axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  # } else {
  #   axis(1,c(1,2,5,9),c("","","",""))
  # }
    axis(1, at = c(12), labels = c("2.5year"))
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
```

```{r}
# Plots of FEVD of Australian rgdp and p

############################################################

hh <- 1:(h + 1)
fevd.au.rgdp <- apply(FEVD.posterior[2,,,], 1:2, mean)
fevd.au.rgdp <- rbind(rep(0, h + 1), apply(fevd.au.rgdp, 2, cumsum))

fevd.realTWI <- apply(FEVD.posterior[3,,,], 1:2, mean)
fevd.realTWI <- rbind(rep(0, h + 1), apply(fevd.realTWI, 2, cumsum))

fevd.exports <- apply(FEVD.posterior[5,,,], 1:2, mean)
fevd.exports <- rbind(rep(0, h + 1), apply(fevd.realTWI, 2, cumsum))
# 
# colors <- c("deepskyblue1","deepskyblue2","deepskyblue","deepskyblue3","deepskyblue4","dodgerblue",
#             "maroon1","maroon","maroon2","magenta","maroon3","maroon4")



# Set up the layout for two plots side by side
 par(mfrow = c(1, 2), mar = rep(4, 4), cex.axis = 1, cex.lab = 0.8)

# Plotting fevd-au-gdp-k1

plot(hh, fevd.au.rgdp[1,], type = "n", ylim = c(0, 100), axes = FALSE, xlab = "", ylab = "")
# Define labels vector for x-axis (axis 1)
x_axis_labels <- rep("", length(hh))  # Start with a vector of empty strings of the same length as 'hh'
x_axis_labels[c(2, 5, 9)] <- c("1 quarter", "1 year", "2 years")  # Place your labels at desired positions

# Replace the labels in your axis() calls with the revised labels vector
axis(1, hh, x_axis_labels)

# axis(1, hh, c("", "1 quarter", "", "", "1 year", "", "", "", "2 years"))
axis(2, c(0, 50, 100), c("", "FEVD[au.rgdp]", ""))
for (n in 1:N) {
  polygon(c(hh, (h + 1):1), c(fevd.au.rgdp[n, hh], fevd.au.rgdp[n + 1, (h + 1):1]), col = colors[n], border = colors[n])
}
# axis(4, (0.5 * (fevd.au.rgdp[1:n, 9] + fevd.au.rgdp[2:n+1, 9]))[c(1, 3, 10)], c("Government spending shock", "", "au.mps"))
axis(4, (0.5 * (fevd.au.rgdp[1:n, 9] + fevd.au.rgdp[2:n+1, 9])))
# Plotting fevd-au-p-k1
plot(hh, fevd.realTWI[1,], type = "n", ylim = c(0, 100), axes = FALSE, xlab = "", ylab = "")
axis(1, hh, x_axis_labels)
axis(2, c(0, 50, 100), c("", "FEVD[realTWI]", ""))
for (n in 1:N) {
  polygon(c(hh, (h + 1):1), c(fevd.realTWI[n, hh], fevd.realTWI[n + 1, (h + 1):1]), col = colors[n], border = colors[n])
}
# axis(4, (0.5 * (fevd.realTWI[1:n, 9] + fevd.realTWI[2:n+1, 9]))[c(3, 10)],)
axis(4, (0.5 * (fevd.realTWI[1:n, 9] + fevd.realTWI[2:n+1, 9])))
```
The key distinction lies in the fact that following the shock, the real exchange rate demonstrates a persistent appreciation and ongoing upward trend. Additionally, the initial response of imports undergoes a noticeable alteration. Based on the provided narrative information, it is feasible to provide a more precise identification of the government spending shock given the information.

## Stochstic Volatility
This section enhance the narrative sign restriction model by incorporating the Stochastic Volatility conditional heteroskedasticity.



```{r}

T             = dim(Y)[1]


H         = diag(T)
sdiag(H,-1) =  -1

HH        = 2*diag(T)
sdiag(HH,-1) =  -1
sdiag(HH,1) =  -1

priors        = list(
  h0.v    = 1,
  sigmav.s= 1,
  sigmav.nu= 1,
  h0.v    = 1,
  h0.m    = 0 ,
  HH      = HH
)


sv1 = SVcommon.Gibbs.iteration(S,Y,X,priors)


  A.posterior= sv1$A
  B.posterior       = array(NA,c(N,N,S))
  B1.posterior = array(NA,c(N,(1+N*p),S))
  for (s in 1:S){
    cholSigma.s     = chol(sv1$Sigma[,,s])
    B.posterior[,,s]= t(cholSigma.s)
    B1.posterior[,,s] =  B.posterior[,,s]%*%t(A.posterior[,,s])
  }
  
sv_posterior = list(A.posterior = A.posterior, B.posterior = B.posterior, Sigma.posterior = sv1$Sigma,B1.posterior = B1.posterior)

SV_Narrative_Restriction = NarrativeSignRestrictions(sign.restrictions = c(1,1,0,0,0,1,1,1), sv_posterior,c(16,50,80,156,204),Y,X)
A.posterior = SV_Narrative_Restriction$A.draws[,,51:800]
B.posterior = SV_Narrative_Restriction$B.draws[,,51:800]

```



The mean and standard deviations for matrix A are reported below:
```{r}
# Calculate the means
Ahcheck_mean <- head(round(apply(A.posterior, 1:2, mean),4))

# Calculate the standard deviations
Ahcheck_sd <- head(round(apply(A.posterior, 1:2, sd),4))
```
```{r,echo=TRUE}
Ahcheck_mean 
Ahcheck_sd

```
The mean and standard deviations for matrix B are reported below:
```{r}
# Calculate the means
Bhcheck_mean <- head(round(apply(B.posterior, 1:2, mean),4))
Bhcheck_sd   <- head(round(apply(B.posterior, 1:2, sd),4))
# Calculate the standard deviations
```
```{r,echo=TRUE}
Bhcheck_mean 
Bhcheck_sd
```
The plot indicates that h reaches to its peak after covid.
```{r}
sv.mean = apply(sv1$H,1,mean)


x.date <- index(df)
x.date <- x.date[6:211]
plot(x=x.date, y=sv.mean, type="l",col=mcxs1,lwd = 2, ylab = "h",xlab = "", main = "Conditional Volatility")

abline(v=as.Date("2020-04-01"), col = "grey25", lty = "dotted", lwd = 1.25)
text(x = as.Date("2019-04-01"), y = max(sv.mean), labels = "COVID", pos = 2)

```

```{r}

par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior[3,,][8,], xlab = "Simulation times S", ylab = expression(A[38]), col = "#CC66CC")    
hist(A.posterior[3,,][8,], xlab = expression(A[38]), col = "#CC66CC", main = '')
plot.ts(B.posterior[3,,][8,], xlab = "Simulation times S", ylab = expression(B[38]), col = "lightblue")    
hist(B.posterior[3,,][8,], xlab = expression(A[38]), col = "lightblue", main = '')

```



```{r}
h = 20
S = 750

IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

for (s in 1:S){
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior


# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1_narrative.RData")

load("irf-fevd-k1_narrative.RData")
IRFs.k1           = apply(IRF.posterior[,1,,],1:2,mean) 

######1st shock is government  spending shock

IRFs.inf.k1       = apply(IRF.inf.posterior[,1,],1,mean)
rownames(IRFs.k1) = colnames(Y)

IRFs.k1.hdi    = apply(IRF.posterior[,1,,],1:2,hdi, credMass=0.68)
hh          = 1:h+1


par(mfrow=c(3,3), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
  # par(mfrow=c(4,2), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  # if (n==N-1 | n==N){
  #   axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  # } else {
  #   axis(1,c(1,2,5,9),c("","","",""))
  # }
    axis(1, at = c(12), labels = c("2.5year"))
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}

```


When we factor in heteroskedasticity, the impulse response function (IRF) experiences substantial alterations. Generally, the shocks to government spending tend to exhibit greater persistence compared to the model without heteroskedasticity. The influence of government spending shocks on imports and exports has been diametrically opposed. An escalation in government spending, depicted by a positive shock, results in a contraction of imports while simultaneously bolstering exports. This shock additionally elicits an upward movement in the Real Trade Weighted Index (TWI). Notably, in contrast to the prior model, imports do not revert to their level prior to the shock.
## References {.unnumbered}



